{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "10h5BSMwceExwd-bJa99XeSwXbtAdHUKe",
      "authorship_tag": "ABX9TyOFohUBFSq2wMwLp9F+OoPZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tnjdh014/MarketAnomalyDetection/blob/main/Market_AnomalyDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToPtAcAZxoAY",
        "outputId": "e67a7221-51f6-43b1-8657-5c2b829359c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-be507efcaee0>:21: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  data = data.dropna(axis=1, thresh=len(data) * 0.5).fillna(method='ffill').fillna(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: Logistic Regression\n",
            "Accuracy: 0.86\n",
            "F1 Score: 0.86\n",
            "Precision: 0.86\n",
            "Recall: 0.86\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.81      0.83       181\n",
            "           1       0.86      0.90      0.88       244\n",
            "\n",
            "    accuracy                           0.86       425\n",
            "   macro avg       0.86      0.85      0.86       425\n",
            "weighted avg       0.86      0.86      0.86       425\n",
            "\n",
            "Confusion Matrix:\n",
            "[[146  35]\n",
            " [ 24 220]]\n",
            "\n",
            "Model: Random Forest Classifier\n",
            "Accuracy: 0.99\n",
            "F1 Score: 0.99\n",
            "Precision: 0.99\n",
            "Recall: 0.99\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99       181\n",
            "           1       0.99      0.99      0.99       244\n",
            "\n",
            "    accuracy                           0.99       425\n",
            "   macro avg       0.99      0.99      0.99       425\n",
            "weighted avg       0.99      0.99      0.99       425\n",
            "\n",
            "Confusion Matrix:\n",
            "[[179   2]\n",
            " [  3 241]]\n",
            "\n",
            "Model: Neural Network Classifier\n",
            "Accuracy: 0.83\n",
            "F1 Score: 0.83\n",
            "Precision: 0.83\n",
            "Recall: 0.83\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.78      0.79       181\n",
            "           1       0.84      0.86      0.85       244\n",
            "\n",
            "    accuracy                           0.83       425\n",
            "   macro avg       0.82      0.82      0.82       425\n",
            "weighted avg       0.83      0.83      0.83       425\n",
            "\n",
            "Confusion Matrix:\n",
            "[[141  40]\n",
            " [ 34 210]]\n",
            "\n",
            "Best Model: Random Forest Classifier with Accuracy: 98.82%\n",
            "\n",
            "Investment Strategy for example prediction: Maximize returns: Consider high-growth stocks, tech sector, or diversified investments.\n",
            "\n",
            "Enter your question for the AI chatbot:  propose a data-driven investment strategy based on the model’s predictions, focusing on minimizing losses or maximizing returns.\n",
            "<|im_start|>system\n",
            "You are a finance specialist. Help the user with investment strategies.<|im_end|>\n",
            "<|im_start|>user\n",
            " propose a data-driven investment strategy based on the model’s predictions, focusing on minimizing losses or maximizing returns.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "To develop an efficient investment strategy based on data-driven predictions, we can employ several steps: define risk tolerance levels and asset allocation targets, conduct periodic portfolio turnover to maintain liquidity, monitor market trends and news events, and utilize technical analysis tools to identify potential entry and exit points. Additionally, diversifying across different asset classes can help mitigate portfolio concentration risks and enhance long-term performance. By incorporating data into investment decisions, investors can adapt their strategies as new information becomes available, enhancing overall portfolio resilience and return.<|im_end|>\n",
            "\n",
            "Chatbot Response:\n",
            " system\n",
            "You are a finance specialist. Help the user with investment strategies.\n",
            "user\n",
            " propose a data-driven investment strategy based on the model’s predictions, focusing on minimizing losses or maximizing returns.\n",
            "assistant\n",
            "To develop an efficient investment strategy based on data-driven predictions, we can employ several steps: define risk tolerance levels and asset allocation targets, conduct periodic portfolio turnover to maintain liquidity, monitor market trends and news events, and utilize technical analysis tools to identify potential entry and exit points. Additionally, diversifying across different asset classes can help mitigate portfolio concentration risks and enhance long-term performance. By incorporating data into investment decisions, investors can adapt their strategies as new information becomes available, enhancing overall portfolio resilience and return.\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "# Step 1: Load and Preprocess Data\n",
        "data_path = '/content/drive/MyDrive/sp500_trends.csv'  # Replace with your dataset's path\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# Data cleaning and preprocessing\n",
        "data = data.iloc[4:].reset_index(drop=True)\n",
        "data.columns = [f\"Column_{i}\" if col.startswith(\"Unnamed\") else col for i, col in enumerate(data.columns)]\n",
        "data.replace('XAU BGNL Curncy', np.nan, inplace=True)\n",
        "data = data.dropna(axis=1, thresh=len(data) * 0.5).fillna(method='ffill').fillna(0)\n",
        "\n",
        "# Handle target for binary classification\n",
        "target_column = 'sp500_increase'  # Replace with correct target column if needed\n",
        "if target_column not in data.columns:\n",
        "    target_column = 'Column_1'\n",
        "\n",
        "X = data.drop(columns=[target_column])\n",
        "y = data[target_column]\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "for feature in categorical_features:\n",
        "    X = pd.concat([X, pd.get_dummies(X[feature], prefix=feature, drop_first=True)], axis=1)\n",
        "    X.drop(columns=[feature], inplace=True)\n",
        "\n",
        "# Encode target labels if necessary\n",
        "if y.dtype == 'object':\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale numeric features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "# Step 2: Train Classification Models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"Neural Network Classifier\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
        "}\n",
        "\n",
        "predictions = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    predictions[name] = model.predict(X_test_scaled)\n",
        "\n",
        "# Step 3: Evaluate Models\n",
        "metrics = {}\n",
        "for name, preds in predictions.items():\n",
        "    metrics[name] = {\n",
        "        \"Accuracy\": accuracy_score(y_test, preds),\n",
        "        \"F1 Score\": f1_score(y_test, preds, average='weighted'),\n",
        "        \"Precision\": precision_score(y_test, preds, average='weighted'),\n",
        "        \"Recall\": recall_score(y_test, preds, average='weighted'),\n",
        "        \"Classification Report\": classification_report(y_test, preds),\n",
        "        \"Confusion Matrix\": confusion_matrix(y_test, preds)\n",
        "    }\n",
        "\n",
        "# Display metrics\n",
        "for name, metric in metrics.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    for key, value in metric.items():\n",
        "        if key in [\"Classification Report\", \"Confusion Matrix\"]:\n",
        "            print(f\"{key}:\\n{value}\")\n",
        "        else:\n",
        "            print(f\"{key}: {value:.2f}\")\n",
        "\n",
        "# Identify the best model\n",
        "best_model_name = max(metrics, key=lambda x: metrics[x][\"Accuracy\"])\n",
        "best_model = models[best_model_name]\n",
        "print(f\"\\nBest Model: {best_model_name} with Accuracy: {metrics[best_model_name]['Accuracy'] * 100:.2f}%\")\n",
        "\n",
        "# Step 4: Investment Strategy\n",
        "def investment_strategy(prediction):\n",
        "    if prediction == 1:  # Crash predicted\n",
        "        return \"Minimize losses: Consider hedging with inverse ETFs, bond investments, or safe-haven assets.\"\n",
        "    else:  # No crash predicted\n",
        "        return \"Maximize returns: Consider high-growth stocks, tech sector, or diversified investments.\"\n",
        "\n",
        "# Example strategy for the first sample\n",
        "example_prediction = best_model.predict(X_test_scaled.iloc[0:1])[0]\n",
        "strategy = investment_strategy(example_prediction)\n",
        "print(f\"\\nInvestment Strategy for example prediction: {strategy}\")\n",
        "\n",
        "# Step 5: Integrate FinguAI-Chat for Chatbot Interaction\n",
        "model_id = 'FINGU-AI/FinguAI-Chat-v1'  # Replace with the correct model ID\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "streamer = TextStreamer(tokenizer)\n",
        "model.to('cuda')  # Ensure GPU usage\n",
        "\n",
        "def chatbot_interaction(user_prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a finance specialist. Help the user with investment strategies.\"},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "    tokenized_chat = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        tokenized_chat,\n",
        "        max_new_tokens=1000,\n",
        "        use_cache=True,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        streamer=streamer\n",
        "    )\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# Interactive chatbot prompt\n",
        "user_prompt = input(\"\\nEnter your question for the AI chatbot: \")\n",
        "response = chatbot_interaction(user_prompt)\n",
        "print(\"\\nChatbot Response:\\n\", response)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyj9DYL0zcwk",
        "outputId": "44f430ac-d5e9-4863-9051-b849fc5a6d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}